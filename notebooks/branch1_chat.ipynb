{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start a chat with LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Branch` object is at the core of lionagi.\n",
    "\n",
    "A branch is an interface to\n",
    "- manages and logs various messages (system, user, assistant) in a conversation, \n",
    "- get AI model inferencing with `Services`\n",
    "- enable effortless multi-round exchange between many participants. \n",
    "- `branch.messages` is a `pd.DataFrame`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lionagi has some default config, you can check them under lionagi.integrations.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lionagi as li\n",
    "from IPython.display import Markdown  # for better print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'gpt-4o',\n",
       " 'frequency_penalty': 0,\n",
       " 'max_tokens': None,\n",
       " 'n': 1,\n",
       " 'presence_penalty': 0,\n",
       " 'response_format': {'type': 'text'},\n",
       " 'seed': None,\n",
       " 'stop': None,\n",
       " 'stream': False,\n",
       " 'temperature': 0.1,\n",
       " 'top_p': 1,\n",
       " 'tools': None,\n",
       " 'tool_choice': 'none',\n",
       " 'user': None,\n",
       " 'logprobs': False,\n",
       " 'top_logprobs': None}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lionagi.integrations.config import oai_schema\n",
    "\n",
    "oai_schema[\"chat/completions\"][\"config\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some messages\n",
    "sys_comedian = \"As a comedian, you are sarcastically funny\"\n",
    "instruct1 = (\n",
    "    \"very short joke: a blue whale and a big shark meet at the bar and start dancing\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a llm conversation with OpenAI gpt-4 (default)\n",
    "comedian1 = li.Branch(system=sys_comedian)\n",
    "joke1 = await comedian1.chat(instruction=instruct1, max_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "And the bartender says, \"Great, now I have to mop up the ocean!\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(joke1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ln_id': '61431747dfbb897a0e829be73615115d',\n",
       " 'timestamp': '2024-05-28T21:35:42.838147',\n",
       " 'provider': 'OpenAI',\n",
       " 'api_key': 'sk-r*******************************************SgD6',\n",
       " 'endpoint': 'chat/completions',\n",
       " 'token_encoding_name': 'cl100k_base',\n",
       " 'model': 'gpt-4o',\n",
       " 'frequency_penalty': 0,\n",
       " 'n': 1,\n",
       " 'presence_penalty': 0,\n",
       " 'response_format': {'type': 'text'},\n",
       " 'stream': False,\n",
       " 'temperature': 0.1,\n",
       " 'top_p': 1,\n",
       " 'tool_choice': 'none',\n",
       " 'logprobs': False,\n",
       " 'model_costs': (5, 15)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comedian1.imodel.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change a LLM provider by choosing a pre-configured service, or create your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imodel = li.iModel(\n",
    "    model=\"mistralai/mistral-7b-instruct\",\n",
    "    provider=\"OpenRouter\",\n",
    "    api_key_schema=\"OPENROUTER_API_KEY\",\n",
    "    costs=(0.07, 0.07),\n",
    ")\n",
    "\n",
    "comedian2 = li.Branch(system=sys_comedian, imodel=imodel)\n",
    "joke2 = await comedian2.chat(instruction=instruct1, max_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ln_id': 'efef4a44e912bc7f6581d4a78b9b673c',\n",
       " 'timestamp': '2024-05-28T21:35:43.950316',\n",
       " 'provider': 'OpenRouter',\n",
       " 'api_key': 'sk-o*****************************************************************4cca',\n",
       " 'endpoint': 'chat/completions',\n",
       " 'token_encoding_name': 'cl100k_base',\n",
       " 'model': 'mistralai/mistral-7b-instruct',\n",
       " 'frequency_penalty': 0,\n",
       " 'presence_penalty': 0,\n",
       " 'response_format': {'type': 'text'},\n",
       " 'stream': False,\n",
       " 'temperature': 0.1,\n",
       " 'top_p': 1,\n",
       " 'tool_choice': 'none',\n",
       " 'logprobs': False,\n",
       " 'model_costs': (0.07, 0.07)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imodel.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The bartender asks, \"What's going on here? Whales can't dance!\"\n",
       "\n",
       "The blue whale replies, \"Well, we're making waves!\"\n",
       "\n",
       "The shark, looking confused, says,"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(joke2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ln_id</th>\n",
       "      <th>message_type</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>role</th>\n",
       "      <th>content</th>\n",
       "      <th>metadata</th>\n",
       "      <th>sender</th>\n",
       "      <th>recipient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77e01e80a0bd0a7ec82000fb4660b62a</td>\n",
       "      <td>System</td>\n",
       "      <td>2024-05-28T21:35:43.950582</td>\n",
       "      <td>system</td>\n",
       "      <td>{'system_info': 'As a comedian, you are sarcas...</td>\n",
       "      <td>{'last_updated': {'recipient': '2024-05-28T21:...</td>\n",
       "      <td>system</td>\n",
       "      <td>e77a6d0e1df796dc22273ecfd81f005b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c0ede8ae13ddc81ab9c285ffaeef2e31</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>2024-05-28T21:35:43.951073</td>\n",
       "      <td>user</td>\n",
       "      <td>{'instruction': 'very short joke: a blue whale...</td>\n",
       "      <td>{'last_updated': {'sender': '2024-05-28T21:35:...</td>\n",
       "      <td>user</td>\n",
       "      <td>e77a6d0e1df796dc22273ecfd81f005b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c19d78ce74fc16973248b246cfd13b64</td>\n",
       "      <td>AssistantResponse</td>\n",
       "      <td>2024-05-28T21:35:45.190993</td>\n",
       "      <td>assistant</td>\n",
       "      <td>{'assistant_response': 'The bartender asks, \"W...</td>\n",
       "      <td>{'last_updated': {'sender': '2024-05-28T21:35:...</td>\n",
       "      <td>e77a6d0e1df796dc22273ecfd81f005b</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              ln_id       message_type  \\\n",
       "0  77e01e80a0bd0a7ec82000fb4660b62a             System   \n",
       "1  c0ede8ae13ddc81ab9c285ffaeef2e31        Instruction   \n",
       "2  c19d78ce74fc16973248b246cfd13b64  AssistantResponse   \n",
       "\n",
       "                    timestamp       role  \\\n",
       "0  2024-05-28T21:35:43.950582     system   \n",
       "1  2024-05-28T21:35:43.951073       user   \n",
       "2  2024-05-28T21:35:45.190993  assistant   \n",
       "\n",
       "                                             content  \\\n",
       "0  {'system_info': 'As a comedian, you are sarcas...   \n",
       "1  {'instruction': 'very short joke: a blue whale...   \n",
       "2  {'assistant_response': 'The bartender asks, \"W...   \n",
       "\n",
       "                                            metadata  \\\n",
       "0  {'last_updated': {'recipient': '2024-05-28T21:...   \n",
       "1  {'last_updated': {'sender': '2024-05-28T21:35:...   \n",
       "2  {'last_updated': {'sender': '2024-05-28T21:35:...   \n",
       "\n",
       "                             sender                         recipient  \n",
       "0                            system  e77a6d0e1df796dc22273ecfd81f005b  \n",
       "1                              user  e77a6d0e1df796dc22273ecfd81f005b  \n",
       "2  e77a6d0e1df796dc22273ecfd81f005b                              user  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comedian2.to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'last_updated': {'sender': '2024-05-28T21:35:45.191074',\n",
       "  'recipient': '2024-05-28T21:35:45.191083'},\n",
       " 'extra': {'id': 'gen-jOFxMAuh3Jh9REs8i5lxM7Rl8nvW',\n",
       "  'model': 'mistralai/mistral-7b-instruct',\n",
       "  'object': 'chat.completion',\n",
       "  'created': 1716932144,\n",
       "  'usage': {'prompt_tokens': 38,\n",
       "   'completion_tokens': 50,\n",
       "   'total_tokens': 88,\n",
       "   'expense': 6.16e-06},\n",
       "  'index': 0,\n",
       "  'finish_reason': 'length'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comedian2.messages[-1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lion_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
