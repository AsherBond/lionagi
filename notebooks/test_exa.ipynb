{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "import json\n",
    "\n",
    "from lionagi import iModel, Branch, types, BaseModel\n",
    "from lionagi.service.providers.exa_.types import ExaSearchRequest\n",
    "from lionagi.utils import alcall\n",
    "\n",
    "\n",
    "class SearchRequests(BaseModel):\n",
    "    search_requests: list[ExaSearchRequest] = []\n",
    "\n",
    "\n",
    "class Analysis(BaseModel):\n",
    "    analysis: str\n",
    "\n",
    "\n",
    "class Source(BaseModel):\n",
    "    title: str\n",
    "    url: str\n",
    "\n",
    "\n",
    "class ResearchDraft(BaseModel):\n",
    "    title: str\n",
    "    content: str\n",
    "    source: list[Source]\n",
    "\n",
    "\n",
    "exa = iModel(\n",
    "    provider=\"exa\",\n",
    "    endpoint=\"search\",\n",
    "    queue_capacity=5,\n",
    "    capacity_refresh_time=1,\n",
    "    invoke_with_endpoint=False,\n",
    ")\n",
    "\n",
    "\n",
    "async def research(\n",
    "    branch: Branch,\n",
    "    query: str,\n",
    "    domain: str | None = None,\n",
    "    style: str | None = None,\n",
    "    sample_writing: str | None = None,\n",
    "    interpret_kwargs: dict | None = None,\n",
    "    *,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    High-level research operation with optional verbose printing:\n",
    "      1) Interpret user query.\n",
    "      2) Generate an analysis from the LLM.\n",
    "      3) Produce search requests and call the EXA provider (cached).\n",
    "      4) Transform search results (compressed text).\n",
    "      5) Prepare a final draft/summary.\n",
    "\n",
    "    Returns an OperableModel with fields:\n",
    "      analysis, search_requests, search_results, transformed_results, draft.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    branch : Branch\n",
    "        The branch instance handling the conversation / context.\n",
    "    query : str\n",
    "        The user's research query.\n",
    "    domain : Optional[str]\n",
    "        Domain hint (e.g. \"finance\", \"marketing\").\n",
    "    style : Optional[str]\n",
    "        Style hint (e.g. \"concise\", \"technical\").\n",
    "    sample_writing : Optional[str]\n",
    "        A sample snippet that might help interpret the style or structure.\n",
    "    interpret_kwargs : Optional[dict]\n",
    "        Additional parameters for the `branch.interpret()` call.\n",
    "    verbose : bool\n",
    "        If True, prints intermediate results step-by-step.\n",
    "    kwargs : dict\n",
    "        Additional arguments passed along if needed.\n",
    "    \"\"\"\n",
    "\n",
    "    out = types.OperableModel()\n",
    "    try:\n",
    "        if interpret_kwargs is None:\n",
    "            interpret_kwargs = {}\n",
    "\n",
    "        # -- Step 1: Interpret the query for better clarity --\n",
    "        interpreted = await branch.interpret(\n",
    "            text=query,\n",
    "            guidance=\"Rewrite the user input to ensure we fully understand and clarify the user's objective.\",\n",
    "            domain=domain,\n",
    "            style=style,\n",
    "            sample=sample_writing,\n",
    "            **interpret_kwargs,\n",
    "        )\n",
    "        if verbose:\n",
    "            display(Markdown(f\"**[Step 1] Interpreted query:** {interpreted}\"))\n",
    "\n",
    "        # -- Step 2: Generate analysis from the LLM --\n",
    "        analysis = await branch.operate(\n",
    "            instruction=interpreted,\n",
    "            guidance=(\n",
    "                \"Perform a thorough analysis focusing on domain knowledge, \"\n",
    "                \"potential angles, and constraints. Be concise but complete.\"\n",
    "            ),\n",
    "            response_format=Analysis,\n",
    "            reason=True,\n",
    "        )\n",
    "        out.add_field(\"analysis\", analysis, annotation=Analysis)\n",
    "        if verbose:\n",
    "            display(\n",
    "                Markdown(f\"**[Step 2] Analysis result:** {analysis.analysis}\")\n",
    "            )\n",
    "\n",
    "        # -- Step 3: Produce search requests based on the analysis --\n",
    "        search_requests: SearchRequests = await branch.operate(\n",
    "            instruction=(\n",
    "                \"Based on the analysis, produce a list of relevant search requests \"\n",
    "                \"for the EXA provider. Focus on the key points from the analysis.\"\n",
    "                \"make sure you get sufficient information from the search results.\"\n",
    "                \"exclude sites like reddit, or other low quality sources.\"\n",
    "            ),\n",
    "            guidance=(\n",
    "                \"Generate specific queries that capture the key aspects from the analysis. \"\n",
    "                \"Provide enough detail for each request.\"\n",
    "            ),\n",
    "            response_format=SearchRequests,\n",
    "            reason=True,\n",
    "        )\n",
    "        out.add_field(\n",
    "            \"search_requests\", search_requests, annotation=SearchRequests\n",
    "        )\n",
    "        if verbose:\n",
    "            display(Markdown(f\"**[Step 3] Search requests:**\"))\n",
    "            for i in search_requests.search_requests:\n",
    "                display(\n",
    "                    Markdown(\n",
    "                        f\"{i.model_dump_json(exclude_none=True, indent=2)}\"\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # Prepare API calls\n",
    "        api_calls = []\n",
    "        for req in search_requests.search_requests:\n",
    "            params = req.model_dump(exclude_none=True)\n",
    "            # Ensure we cache search results by default\n",
    "            params[\"is_cached\"] = params.get(\"is_cached\", True)\n",
    "            api_call = exa.create_api_calling(**params)\n",
    "            api_calls.append(api_call)\n",
    "\n",
    "        # Invoke EXA searches asynchronously\n",
    "        search_results = await alcall(\n",
    "            api_calls, exa.invoke, retry_default=None, dropna=True\n",
    "        )\n",
    "        out.add_field(\n",
    "            \"search_results\",\n",
    "            [res.response for res in search_results],\n",
    "            annotation=list[dict],\n",
    "        )\n",
    "        if verbose:\n",
    "            display(Markdown(f\"**[Step 3] Search results:**\"))\n",
    "            for res in search_results:\n",
    "                display(\n",
    "                    Markdown(\n",
    "                        \"\\n\".join(\n",
    "                            \"  - \" + i[\"title\"]\n",
    "                            for i in res.response[\"results\"]\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # -- Step 4: Draft a final output referencing the transformed results --\n",
    "        draft = await branch.operate(\n",
    "            instruction=(\n",
    "                \"Prepare a well-formatted, factual research report basing on the research findings. \"\n",
    "                \"Incorporate key insights from the context.\"\n",
    "            ),\n",
    "            guidance=(\n",
    "                \"Synthesize a final summary using all insights gleaned from the search results. \"\n",
    "                \"Ensure clarity and accuracy, and follow any requested style.\"\n",
    "                \"answer the user's question, and provide additional context.\"\n",
    "            ),\n",
    "            context={\n",
    "                \"search_results\": json.dumps(\n",
    "                    [res.response for res in search_results]\n",
    "                )\n",
    "            },\n",
    "            response_format=ResearchDraft,\n",
    "        )\n",
    "        out.add_field(\"research_draft\", draft, annotation=ResearchDraft)\n",
    "        if verbose:\n",
    "            display(Markdown(f\"**[Step 4] Draft:** \\n\\n{draft.content}\"))\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        out.add_field(\"error\", str(e), annotation=str)\n",
    "        print(\"Error occurred during research:\", e)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "researcher_prompt = \"\"\"SYSTEM PROMPT (Researcher):\n",
    "You are a specialized research assistant, trained to gather information from various sources accurately and concisely. Your job involves:\n",
    " • Interpreting user questions and clarifying objectives,\n",
    " • Proposing relevant angles or methods of inquiry,\n",
    " • Generating precise search queries to explore any topic,\n",
    " • Summarizing findings accurately while preserving key details.\n",
    "\n",
    "When performing your tasks:\n",
    " • Confirm context and constraints (like domain or style requirements).\n",
    " • Provide well-structured, consistent, and thorough analyses.\n",
    " • Use suitable search queries to gather relevant info.\n",
    " • Summarize or compress results in a way that remains factual.\n",
    " • Maintain an objective, knowledgeable, and professional tone.\n",
    "\n",
    "Overall, your responsibility is to produce high-quality research findings and drafts that help the user solve problems or gather insights effectively.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of `research` function\n",
    "\n",
    "from lionagi.session.branch import Branch\n",
    "\n",
    "haiku = iModel(\n",
    "    provider=\"openrouter\",\n",
    "    model=\"anthropic/claude-3.5-haiku\",\n",
    "    max_tokens=8000,  # required for anthropic models\n",
    "    invoke_with_endpoint=False,\n",
    ")\n",
    "\n",
    "sonnet = iModel(\n",
    "    provider=\"openrouter\",\n",
    "    model=\"anthropic/claude-3.5-sonnet\",\n",
    "    max_tokens=8000,  # required for anthropic models\n",
    "    invoke_with_endpoint=False,\n",
    ")\n",
    "\n",
    "researcher = Branch(\n",
    "    system=researcher_prompt,\n",
    "    chat_model=sonnet,\n",
    "    parse_model=haiku,\n",
    ")\n",
    "\n",
    "# Example query requesting an analysis of LLM-based summarization in finance\n",
    "query_text = (\n",
    "    \"I want to compare different LLM-based summarization approaches \"\n",
    "    \"for financial documents. Focus on accuracy, cost, and domain adaptability. \"\n",
    "    \"Also highlight practical use-cases or references.\"\n",
    ")\n",
    "\n",
    "# Optional style or domain hints\n",
    "domain_hint = \"finance\"\n",
    "style_hint = \"extensive\"\n",
    "sample_snippet = (\n",
    "    \"Sample text: In the finance domain, we often handle massive amounts of data. \"\n",
    "    \"We want a method that can summarize quickly and accurately.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**[Step 1] Interpreted query:** Here's the clarified and structured prompt:\n",
       "\n",
       "Please provide a comprehensive analysis of Large Language Model (LLM) approaches for financial document summarization, addressing the following key aspects:\n",
       "\n",
       "1. Comparison of different LLM summarization methods:\n",
       "   - Zero-shot vs. few-shot approaches\n",
       "   - Fine-tuned models vs. general-purpose models\n",
       "   - Extractive vs. abstractive summarization techniques\n",
       "\n",
       "2. Evaluation criteria:\n",
       "   - Accuracy metrics (ROUGE scores, human evaluation)\n",
       "   - Cost considerations (token usage, computational resources)\n",
       "   - Domain adaptation capabilities for financial texts\n",
       "   - Handling of specialized financial terminology and numerical data\n",
       "\n",
       "3. Practical implementation considerations:\n",
       "   - Real-world use cases in financial institutions\n",
       "   - Regulatory compliance and data security requirements\n",
       "   - Integration with existing document management systems\n",
       "   - Scalability for large document volumes\n",
       "\n",
       "4. Specific financial document types to consider:\n",
       "   - Annual reports (10-K, 10-Q)\n",
       "   - Financial news articles\n",
       "   - Research reports\n",
       "   - Regulatory filings\n",
       "   - Earnings call transcripts\n",
       "\n",
       "Please include relevant academic studies, industry implementations, or benchmark results where available."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[Step 2] Analysis result:** Domain Knowledge Analysis:\n",
       "\n",
       "1. LLM Summarization Methods\n",
       "- Zero-shot vs Few-shot:\n",
       "  • Zero-shot better for standardized documents (10-K reports)\n",
       "  • Few-shot excels with domain-specific terminology\n",
       "  • Few-shot requires carefully curated examples\n",
       "\n",
       "- Model Selection:\n",
       "  • Fine-tuned models show superior performance on financial texts\n",
       "  • Domain-adapted models better handle numerical data\n",
       "  • General models struggle with financial jargon\n",
       "\n",
       "- Summarization Techniques:\n",
       "  • Extractive better preserves numerical accuracy\n",
       "  • Abstractive provides more coherent narratives\n",
       "  • Hybrid approaches show promise for financial documents\n",
       "\n",
       "2. Key Constraints:\n",
       "\n",
       "Technical:\n",
       "- Token limits impact long document processing\n",
       "- GPU memory requirements for large models\n",
       "- Real-time processing needs\n",
       "\n",
       "Regulatory:\n",
       "- Data privacy (GDPR, CCPA)\n",
       "- Audit trail requirements\n",
       "- Model explainability needs\n",
       "\n",
       "Domain-specific:\n",
       "- Numerical accuracy preservation\n",
       "- Financial terminology handling\n",
       "- Temporal context maintenance\n",
       "\n",
       "3. Implementation Considerations:\n",
       "\n",
       "Practical:\n",
       "- Batch processing for large document volumes\n",
       "- API integration capabilities\n",
       "- Error handling protocols\n",
       "\n",
       "Scalability:\n",
       "- Distributed processing architecture\n",
       "- Load balancing requirements\n",
       "- Storage optimization needs\n",
       "\n",
       "4. Document-Specific Approaches:\n",
       "\n",
       "Annual Reports:\n",
       "- Section-wise processing\n",
       "- Key metrics extraction\n",
       "- Year-over-year comparison focus\n",
       "\n",
       "Earnings Calls:\n",
       "- Speaker diarization handling\n",
       "- Q&A section summarization\n",
       "- Forward-looking statement identification\n",
       "\n",
       "Research Reports:\n",
       "- Citation preservation\n",
       "- Methodology extraction\n",
       "- Findings prioritization\n",
       "\n",
       "5. Evaluation Framework:\n",
       "\n",
       "Accuracy Metrics:\n",
       "- ROUGE scores for content alignment\n",
       "- Domain-specific metrics for financial accuracy\n",
       "- Human evaluation for practical usability\n",
       "\n",
       "Performance Metrics:\n",
       "- Processing time per document\n",
       "- Resource utilization\n",
       "- Cost per summary\n",
       "\n",
       "6. Recommendations:\n",
       "\n",
       "- Implement hybrid approach combining extractive and abstractive methods\n",
       "- Use fine-tuned models for specialized documents\n",
       "- Establish robust validation pipeline\n",
       "- Include human-in-the-loop for critical documents\n",
       "- Regular model retraining with domain updates"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[Step 3] Search requests:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "{\n",
       "  \"query\": \"comparison zero shot few shot fine-tuning LLM financial document summarization evaluation metrics ROUGE scores\",\n",
       "  \"category\": \"research paper\",\n",
       "  \"type\": \"neural\",\n",
       "  \"useAutoprompt\": false,\n",
       "  \"numResults\": 15,\n",
       "  \"excludeDomains\": [\n",
       "    \"reddit.com\",\n",
       "    \"medium.com\",\n",
       "    \"quora.com\"\n",
       "  ],\n",
       "  \"startPublishedDate\": \"2022-01-01T00:00:00.000Z\",\n",
       "  \"contents\": {\n",
       "    \"highlights\": {\n",
       "      \"highlightsPerUrl\": 1,\n",
       "      \"numSentences\": 3\n",
       "    },\n",
       "    \"summary\": {},\n",
       "    \"livecrawl\": \"never\",\n",
       "    \"livecrawlTimeout\": 10000\n",
       "  }\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "{\n",
       "  \"query\": \"financial document LLM summarization case studies banks investment firms implementation challenges\",\n",
       "  \"category\": \"research paper\",\n",
       "  \"type\": \"neural\",\n",
       "  \"useAutoprompt\": false,\n",
       "  \"numResults\": 10,\n",
       "  \"excludeDomains\": [\n",
       "    \"reddit.com\",\n",
       "    \"medium.com\",\n",
       "    \"quora.com\"\n",
       "  ],\n",
       "  \"startPublishedDate\": \"2022-01-01T00:00:00.000Z\",\n",
       "  \"includeText\": [\n",
       "    \"case study\",\n",
       "    \"implementation\"\n",
       "  ],\n",
       "  \"contents\": {\n",
       "    \"highlights\": {\n",
       "      \"highlightsPerUrl\": 1,\n",
       "      \"numSentences\": 3\n",
       "    },\n",
       "    \"summary\": {},\n",
       "    \"livecrawl\": \"never\",\n",
       "    \"livecrawlTimeout\": 10000\n",
       "  }\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "{\n",
       "  \"query\": \"regulatory compliance data security requirements LLM financial document processing GDPR CCPA\",\n",
       "  \"category\": \"research paper\",\n",
       "  \"type\": \"neural\",\n",
       "  \"useAutoprompt\": false,\n",
       "  \"numResults\": 10,\n",
       "  \"excludeDomains\": [\n",
       "    \"reddit.com\",\n",
       "    \"medium.com\",\n",
       "    \"quora.com\"\n",
       "  ],\n",
       "  \"startPublishedDate\": \"2022-01-01T00:00:00.000Z\",\n",
       "  \"contents\": {\n",
       "    \"highlights\": {\n",
       "      \"highlightsPerUrl\": 1,\n",
       "      \"numSentences\": 3\n",
       "    },\n",
       "    \"summary\": {},\n",
       "    \"livecrawl\": \"never\",\n",
       "    \"livecrawlTimeout\": 10000\n",
       "  }\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "{\n",
       "  \"query\": \"extractive vs abstractive summarization financial reports numerical data preservation techniques\",\n",
       "  \"category\": \"research paper\",\n",
       "  \"type\": \"neural\",\n",
       "  \"useAutoprompt\": false,\n",
       "  \"numResults\": 10,\n",
       "  \"excludeDomains\": [\n",
       "    \"reddit.com\",\n",
       "    \"medium.com\",\n",
       "    \"quora.com\"\n",
       "  ],\n",
       "  \"startPublishedDate\": \"2022-01-01T00:00:00.000Z\",\n",
       "  \"contents\": {\n",
       "    \"highlights\": {\n",
       "      \"highlightsPerUrl\": 1,\n",
       "      \"numSentences\": 3\n",
       "    },\n",
       "    \"summary\": {},\n",
       "    \"livecrawl\": \"never\",\n",
       "    \"livecrawlTimeout\": 10000\n",
       "  }\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "{\n",
       "  \"query\": \"technical architecture scalability large language models financial document processing distributed systems\",\n",
       "  \"category\": \"research paper\",\n",
       "  \"type\": \"neural\",\n",
       "  \"useAutoprompt\": false,\n",
       "  \"numResults\": 10,\n",
       "  \"excludeDomains\": [\n",
       "    \"reddit.com\",\n",
       "    \"medium.com\",\n",
       "    \"quora.com\"\n",
       "  ],\n",
       "  \"startPublishedDate\": \"2022-01-01T00:00:00.000Z\",\n",
       "  \"contents\": {\n",
       "    \"highlights\": {\n",
       "      \"highlightsPerUrl\": 1,\n",
       "      \"numSentences\": 3\n",
       "    },\n",
       "    \"summary\": {},\n",
       "    \"livecrawl\": \"never\",\n",
       "    \"livecrawlTimeout\": 10000\n",
       "  }\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:API call to https://api.exa.ai/search failed: catching classes that do not inherit from BaseException is not allowed\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**[Step 3] Search results:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "  - Combining State-of-the-Art Models with Maximal Marginal Relevance for Few-Shot and Zero-Shot Multi-Document Summarization\n",
       "  - A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment Analysis\n",
       "  - Benchmarking Large Language Models for News Summarization\n",
       "  - Open-source Large Language Models are Strong Zero-shot Query Likelihood Models for Document Ranking\n",
       "  - Less is More for Long Document Summary Evaluation by LLMs\n",
       "  - Low-Resource Court Judgment Summarization for Common Law Systems\n",
       "  - Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation\n",
       "  - Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately\n",
       "  - UniSumEval: Towards Unified, Fine-Grained, Multi-Dimensional Summarization Evaluation for LLMs\n",
       "  - DocAsRef: A Pilot Empirical Study on Repurposing Reference-Based Summary Quality Metrics Reference-Freely\n",
       "  - Multi-Document Financial Question Answering using LLMs\n",
       "  - Improving Zero-Shot Text Matching for Financial Auditing with Large Language Models\n",
       "  - Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems\n",
       "  - Financial Knowledge Large Language Model\n",
       "  - Towards Optimizing the Costs of LLM Usage"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "  - Documentation\n",
       "  - Building data management capabilities to address data protection regulations: Learnings from EU-GDPR\n",
       "  - Best Practices for CCPA & GDPR Compliance | Micro Focus\n",
       "  - AI-Enabled Automation for Completeness Checking of Privacy Policies\n",
       "  - Data protection rules applicable to Financial Intelligence Units: still no clarity in sight\n",
       "  - Support for Enhanced GDPR Accountability with the Common Semantic Model for ROPA (CSM-ROPA)\n",
       "  - Data Security on the Ground: Investigating Technical and Legal Requirements under the GDPR\n",
       "  - Know Your Customer: Balancing innovation and regulation for financial inclusion | Data & Policy | Cambridge Core\n",
       "  - GDPR and unstructured data: is anonymization possible?\n",
       "  - Data Protection Act 2018"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "  - Numerical Reasoning for Financial Reports\n",
       "  - Long Text and Multi-Table Summarization: Dataset and Method\n",
       "  - L3iTC at the FinLLM Challenge Task: Quantization for Financial Text Classification & Summarization\n",
       "  - Information Extraction through AI techniques: The KIDs use case at CONSOB\n",
       "  - Towards reducing hallucination in extracting information from financial reports using Large Language Models\n",
       "  - NumHTML: Numeric-Oriented Hierarchical Transformer Model for Multi-task Financial Forecasting\n",
       "  - Leveraging LLMs for KPIs Retrieval from Hybrid Long-Document: A Comprehensive Framework and Dataset\n",
       "  - Leveraging LLMs for KPIs Retrieval from Hybrid Long-Document: A Comprehensive Framework and Dataset\n",
       "  - Financial Report Chunking for Effective Retrieval Augmented Generation\n",
       "  - REFinD: Relation Extraction Financial Dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "  - DocFinQA: A Long-Context Financial Reasoning Dataset\n",
       "  - Distributed Inference and Fine-tuning of Large Language Models Over The Internet\n",
       "  - Enabling Cross-Language Data Integration and Scalable Analytics in Decentralized Finance\n",
       "  - Efficient Parallelization Layouts for Large-Scale Distributed Model Training\n",
       "  - Delivering Document Conversion as a Cloud Service with High Throughput and Responsiveness\n",
       "  - LongFin: A Multimodal Document Understanding Model for Long Financial Domain Documents\n",
       "  - Shai: A large language model for asset management\n",
       "  - DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving\n",
       "  - Chiplet Cloud: Building AI Supercomputers for Serving Large Generative Language Models\n",
       "  - Paper page - MegaScale: Scaling Large Language Model Training to More Than 10,000\n",
       "  GPUs"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[Step 4] Draft:** \n",
       "\n",
       "This research report synthesizes current approaches and best practices for using Large Language Models (LLMs) in financial document summarization.\n",
       "\n",
       "Key Findings:\n",
       "\n",
       "1. Summarization Methods\n",
       "- Zero-shot vs Few-shot: Recent studies show few-shot learning performs better with domain-specific financial terminology, while zero-shot works well for standardized documents like 10-K reports. One study found increasing the number of few-shot examples did not significantly improve performance.\n",
       "\n",
       "- Fine-tuning vs General Models: Research demonstrates fine-tuned models achieve superior performance on financial texts, particularly for preserving numerical accuracy. Fine-tuned smaller models (250M-3B parameters) can match larger models' performance with domain-specific training.\n",
       "\n",
       "- Extractive vs Abstractive: Extractive methods better preserve numerical data accuracy, while abstractive provides more coherent narratives. Hybrid approaches combining both show promising results.\n",
       "\n",
       "2. Accuracy and Evaluation\n",
       "- Numerical Data Preservation: Studies highlight the importance of preserving numerical accuracy in financial summaries. New evaluation frameworks like FINE dataset specifically test numerical extraction accuracy.\n",
       "- ROUGE Scores: While commonly used, research suggests ROUGE scores alone are insufficient for financial documents. Multiple studies recommend complementary metrics focused on numerical accuracy.\n",
       "- Human Evaluation: Recent work emphasizes the need for domain expert evaluation, particularly for assessing factual accuracy in financial contexts.\n",
       "\n",
       "3. Technical Implementation\n",
       "- Scalability: Distributed processing architectures show promise for handling large document volumes, with some implementations processing over 1M pages per hour.\n",
       "- Data Security: Research emphasizes implementing robust security measures meeting GDPR/CCPA requirements, particularly for financial data processing.\n",
       "- System Integration: Studies recommend hybrid approaches combining LLM capabilities with existing document management systems.\n",
       "\n",
       "Recommendations:\n",
       "1. Implement hybrid summarization approaches combining extractive methods for numerical data with abstractive techniques for narrative sections\n",
       "2. Use domain-adapted fine-tuned models for specialized financial documents\n",
       "3. Establish multi-metric evaluation frameworks including numerical accuracy metrics\n",
       "4. Deploy distributed processing architecture for handling large document volumes\n",
       "5. Ensure compliance with data protection regulations through appropriate security measures\n",
       "\n",
       "Limitations and Future Work:\n",
       "- Need for improved methods to handle extremely long financial documents\n",
       "- Further research required on maintaining temporal context in multi-period financial reports\n",
       "- Development of standardized evaluation metrics for financial document summarization"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now call your research function\n",
    "result = await research(\n",
    "    branch=researcher,\n",
    "    query=query_text,\n",
    "    domain=domain_hint,\n",
    "    style=style_hint,\n",
    "    sample_writing=sample_snippet,\n",
    "    interpret_kwargs={\"temperature\": 0.3},  # example\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Source:** [A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment Analysis](https://arxiv.org/abs/2312.08725)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Source:** [Leveraging LLMs for KPIs Retrieval from Hybrid Long-Document: A Comprehensive Framework and Dataset](https://arxiv.org/abs/2305.16344)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Source:** [Numerical Reasoning for Financial Reports](http://arxiv.org/abs/2312.14870)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Source:** [Data Protection Act 2018](https://www.legislation.gov.uk/ukpga/2018/12/contents)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Source:** [DocFinQA: A Long-Context Financial Reasoning Dataset](https://arxiv.org/abs/2401.06915)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in result.research_draft.source:\n",
    "    display(Markdown(f\"**Source:** [{i.title}]({i.url})\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
