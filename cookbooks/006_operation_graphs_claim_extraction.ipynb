{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 006: Operation Graphs - Academic Claim Validation\n",
    "\n",
    "Building on ReAct patterns from tutorial 005, this demonstrates sequential coordination using Operation Graphs to orchestrate ReaderTool workflows for academic claim validation.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Sequential Coordination**: Building context step-by-step through dependent operations\n",
    "2. **ReaderTool Integration**: Document chunking and progressive reading strategies  \n",
    "3. **Structured Extraction**: Using Pydantic models for reliable claim extraction\n",
    "4. **Operation Dependencies**: How operations build on previous results\n",
    "\n",
    "## Use Case: Validating a Theoretical Framework Paper\n",
    "\n",
    "We'll validate claims in an academic paper about capability-based security by:\n",
    "- Reading document chunks progressively with ReaderTool\n",
    "- Building understanding through sequential analysis\n",
    "- Extracting verifiable claims with structured output formats\n",
    "- Demonstrating how each operation builds on the previous one's results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment setup complete\n",
      "ðŸ“„ Target: 006_lion_proof_ch2.md\n",
      "ðŸŽ¯ Goal: Validate academic claims using coordinated ReAct workflows\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "from typing import Literal\n",
    "from pathlib import Path\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from lionagi import Branch, Session, Builder, types, iModel\n",
    "from lionagi.tools.types import ReaderTool\n",
    "\n",
    "# Target document - complex theoretical framework\n",
    "here = Path().cwd()\n",
    "document_path = here / \"data\" / \"006_lion_proof_ch2.md\"\n",
    "\n",
    "print(\"âœ… Environment setup complete\")\n",
    "print(f\"ðŸ“„ Target: {document_path.name}\")\n",
    "print(\"ðŸŽ¯ Goal: Validate academic claims using coordinated ReAct workflows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data models defined\n"
     ]
    }
   ],
   "source": [
    "# Data models for structured responses\n",
    "class Claim(BaseModel):\n",
    "    claim: str\n",
    "    type: Literal[\"citation\", \"performance\", \"technical\", \"other\"]\n",
    "    location: str = Field(..., description=\"Section/paragraph reference\")\n",
    "    verifiability: Literal[\"high\", \"medium\", \"low\"]\n",
    "    search_strategy: str = Field(..., description=\"How to verify this claim\")\n",
    "\n",
    "\n",
    "class ClaimExtraction(BaseModel):\n",
    "    claims: list[Claim]\n",
    "\n",
    "\n",
    "print(\"âœ… Data models defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 1: Sequential Document Analysis\n",
    "\n",
    "Build understanding step-by-step: Open â†’ Analyze â†’ Extract claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— Executing sequential analysis...\n",
      "Executing operation: eaa2dd33-fb60-435c-ab9b-c6991e4a8fcc\n",
      "Invoking action reader_tool with {'action': 'open', 'path_or_url': '/Users/lion/lio....\n",
      "Action reader_tool invoked, status: completed.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### ReAct Round No.1 Analysis:\n",
       "```yaml\n",
       "analysis: I will first open the theoretical framework document to analyze its overall structure. After opening, I will read through the main sections to detect which parts contain verifiable c...\n",
       "\n",
       "[Truncated output]\n",
       "```\n",
       "---------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ReAct Final Answer:\n",
       "```yaml\n",
       "\n",
       "```\n",
       "---------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing operation: 2b1894b1-8a11-4c7d-8107-349bc0254cee\n",
      "Invoking action reader_tool with {'action': 'read', 'doc_id': 'DOC_9221526026981911....\n",
      "Action reader_tool invoked, status: completed.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### ReAct Round No.1 Analysis:\n",
       "```yaml\n",
       "analysis: Given the length of the document (52,767 characters), we need to selectively read portions likely to contain verifiable citations, technical claims, or performance results. Typically...\n",
       "\n",
       "[Truncated output]\n",
       "```\n",
       "---------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking action reader_tool with {'action': 'read', 'doc_id': 'DOC_9221526026981911....\n",
      "Action reader_tool invoked, status: completed.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### ReAct Round No.2 Analysis:\n",
       "```yaml\n",
       "analysis: The initial 4000 characters confirm key theorems and formal definitions pertinent to the Lion ecosystem's security model. To thoroughly identify citations, technical claims, and perf...\n",
       "\n",
       "[Truncated output]\n",
       "```\n",
       "---------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking action reader_tool with {'action': 'read', 'doc_id': 'DOC_9221526026981911....\n",
      "Action reader_tool invoked, status: completed.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### ReAct Round No.3 Analysis:\n",
       "```yaml\n",
       "analysis: The last reading chunk included the formal statement and part of the proof structure for Theorem 2.1 regarding cross-component capability flow preservation. To fully understand relat...\n",
       "\n",
       "[Truncated output]\n",
       "```\n",
       "---------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking action reader_tool with {'action': 'read', 'doc_id': 'DOC_9221526026981911....\n",
      "Action reader_tool invoked, status: completed.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### ReAct Round No.4 Analysis:\n",
       "```yaml\n",
       "analysis: The previous chunk provided the completion of Lemma 2.1.2 and a sketch for Lemma 2.1.3, and began the conclusion of Theorem 2.1. To ensure a comprehensive understanding of the proof,...\n",
       "\n",
       "[Truncated output]\n",
       "```\n",
       "---------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ReAct Round No.5 Analysis:\n",
       "```yaml\n",
       "analysis: The last chunk read completes the proof of Theorem 2.1, including its three supporting lemmas which cover unforgeable capability references via WebAssembly isolation, preservation of...\n",
       "\n",
       "[Truncated output]\n",
       "```\n",
       "---------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ReAct Final Answer:\n",
       "```yaml\n",
       "topic: Analysis of Theoretical Framework Document for Verifiable Citations, Technical Claims, and Performance Metrics\n",
       "items:\n",
       "  - heading: 2.1 Introduction\n",
       "    summary: Introduces the Lion ecos...\n",
       "\n",
       "[Truncated output]\n",
       "```\n",
       "---------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing operation: b28df347-71f2-476c-8e7f-9f6740d4c3c0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### ReAct Round No.1 Analysis:\n",
       "```yaml\n",
       "analysis: Based on the analysis of the theoretical framework document, six specific verifiable claims have been extracted, focusing on formal theorems, implementation details, and mechanized p...\n",
       "\n",
       "[Truncated output]\n",
       "```\n",
       "---------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ReAct Final Answer:\n",
       "```yaml\n",
       "claims:\n",
       "  - claim: Theorem 2.1: Cross-Component Capability Flow Preservation states that capability authority is preserved across component boundaries, and capability references remain unforge...\n",
       "\n",
       "[Truncated output]\n",
       "```\n",
       "---------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "async def sequential_analysis():\n",
    "    \"\"\"Sequential workflow: open â†’ analyze structure â†’ extract claims.\"\"\"\n",
    "\n",
    "    # Create branch with ReaderTool\n",
    "    branch = Branch(\n",
    "        tools=[ReaderTool], chat_model=iModel(model=\"openai/gpt-4.1-mini\")\n",
    "    )\n",
    "    session = Session(default_branch=branch)\n",
    "    builder = Builder()\n",
    "\n",
    "    # Step 1: Open and understand document\n",
    "    doc_reader = builder.add_operation(\n",
    "        \"ReAct\",\n",
    "        node_id=\"open_document\",\n",
    "        instruct=types.Instruct(\n",
    "            instruction=\"Use ReaderTool to open and analyze the theoretical framework document. Understand its structure and identify sections containing verifiable claims.\",\n",
    "            context={\"document_path\": str(document_path)},\n",
    "        ),\n",
    "        tools=[\"reader_tool\"],\n",
    "        max_extensions=2,\n",
    "        verbose=True,\n",
    "        verbose_length=200,\n",
    "    )\n",
    "\n",
    "    # Step 2: Progressive content analysis\n",
    "    content_analyzer = builder.add_operation(\n",
    "        \"ReAct\",\n",
    "        node_id=\"analyze_content\",\n",
    "        depends_on=[doc_reader],\n",
    "        instruct=types.Instruct(\n",
    "            instruction=\"Read through key sections to identify citations, technical claims, and performance metrics that can be verified.\"\n",
    "        ),\n",
    "        response_format=types.Outline,\n",
    "        tools=[\"reader_tool\"],\n",
    "        max_extensions=3,\n",
    "        verbose=True,\n",
    "        verbose_length=200,\n",
    "    )\n",
    "\n",
    "    # Step 3: Extract specific claims\n",
    "    claim_extractor = builder.add_operation(\n",
    "        \"ReAct\",\n",
    "        node_id=\"extract_claims\",\n",
    "        depends_on=[content_analyzer],\n",
    "        instruct=types.Instruct(\n",
    "            instruction=\"Extract 5-7 specific, verifiable claims. Prioritize citations, performance metrics, and technical assertions.\"\n",
    "        ),\n",
    "        response_format=ClaimExtraction,\n",
    "        tools=[\"reader_tool\"],\n",
    "        max_extensions=3,\n",
    "        verbose=True,\n",
    "        verbose_length=200,\n",
    "    )\n",
    "\n",
    "    # Execute workflow\n",
    "    graph = builder.get_graph()\n",
    "    print(\"ðŸ”— Executing sequential analysis...\")\n",
    "\n",
    "    result = await session.flow(graph, parallel=False, verbose=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Execute sequential analysis\n",
    "result = await sequential_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## ðŸ“„ Document Structure (2b1894b1-8a11-4c7d-8107-349bc0254cee)\n",
       "\n",
       "**Topic:** Analysis of Theoretical Framework Document for Verifiable Citations, Technical Claims, and Performance Metrics\n",
       "\n",
       "### Key Sections:\n",
       "- **2.1 Introduction**: Introduces the Lion ecosystem's novel capability-based security approach, motivating the work and outlining four main theorems with formal proofs and their integration with mechanized models and Rust implementation. Contains key technical claims about security challenges addressed and innovative contributions.\n",
       "- **2.2 System Model and Formal Definitions**: Defines the Lion ecosystem architecture and formalizes the capability system as a mathematical model with sets for capabilities, rights, objects, subjects, policies, isolation contexts, and communication functions. Introduces formal definitions of capabilities, authority, component composition, and security properties. These definitions contain precise, verifiable technical claims foundational to subsequent theorems.\n",
       "- **2.3 Theorem 2.1: Cross-Component Capability Flow**: States and formally proves Theorem 2.1, claiming capability authority preservation and unforgeability across component boundaries. Proof is divided into three lemmas: WebAssembly isolation enforcing reference integrity, capability transfer protocol ensuring authority preservation (with Rust code snippets providing implementation details), and policy compliance during transfer. Also notes mechanization of proofs in TLA+. This section holds multiple verifiable technical claims and partial implementation evidence, though no explicit performance metrics.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "## ðŸ“‘ Extracted Claims (b28df347-71f2-476c-8e7f-9f6740d4c3c0)\n",
       "\n",
       "Found **6** verifiable claims:\n",
       "\n",
       "\n",
       "### 1. [TECHNICAL] Theorem 2.1: Cross-Component Capability Flow Preservation states that capability authority is preserved across component boundaries, and capability references remain unforgeable during inter-component communication.\n",
       "\n",
       "- **Location:** Section 2.3, Theorem 2.1 and subsections 2.3.1â€“2.3.3  \n",
       "- **Verifiability:** high\n",
       "- **Search Strategy:** Verify formal definitions and proofs presented in the document, examine mechanized TLA+ model (Appendix A.2), and review formal lemma proofs and Rust code snippets demonstrating unforgeability and authority preservation.\n",
       "\n",
       "\n",
       "### 2. [TECHNICAL] Lemma 2.1.1 demonstrates that WebAssembly isolation preserves capability reference integrity by separating host memory and sandboxed linear memory, using cryptographically secure, injective handles unforgeable by WebAssembly modules independently.\n",
       "\n",
       "- **Location:** Section 2.3.2, Lemma 2.1.1  \n",
       "- **Verifiability:** high\n",
       "- **Search Strategy:** Review the formal memory separation model and cryptographic assumptions described, validate the injective and unforgeable properties of handles through the mathematical definitions and supporting proofs.\n",
       "\n",
       "\n",
       "### 3. [TECHNICAL] The capability transfer protocol serializes capability objects into handles with HMAC signatures ensuring integrity during transfer, as shown in Rust code examples of serialize_capability and deserialize_capability functions within lion core.\n",
       "\n",
       "- **Location:** Section 2.3.2, Lemma 2.1.2  \n",
       "- **Verifiability:** high\n",
       "- **Search Strategy:** Examine the provided Rust code snippets and cryptographic signature usage, confirm implementation correctness by matching with protocol descriptions and verifying signature enforcement.\n",
       "\n",
       "\n",
       "### 4. [TECHNICAL] All capability transfers are mediated by the lion policy component which enforces that transfers only occur if allowed by policy, modeled formally as send(s1,s2,c) implying policy_allows(s1,s2,c), ensuring policy compliance.\n",
       "\n",
       "- **Location:** Section 2.3.2, Lemma 2.1.3  \n",
       "- **Verifiability:** medium\n",
       "- **Search Strategy:** Review formal policy enforcement descriptions, analyze the lion policy component behavior, and verify policy checks during capability transfer via system code or documentation.\n",
       "\n",
       "\n",
       "### 5. [TECHNICAL] Theorem 2.2: Security Composition Preservation asserts that composition of individually secure, compatible Lion components preserves overall system security, including unforgeable refs, authority confinement, least privilege, and policy compliance.\n",
       "\n",
       "- **Location:** Section 2.4, Theorem 2.2 and subsections 2.4.1â€“2.4.2  \n",
       "- **Verifiability:** high\n",
       "- **Search Strategy:** Verify formal theorem statements and lemma proofs related to composition, assess logical reasoning provided, and validate through theoretical model or mechanization if available.\n",
       "\n",
       "\n",
       "### 6. [CITATION] Mechanized proofs and model checking of the Lion ecosystem's capability flow and security properties are conducted using TLA+, specifically for Theorem 2.1 and related lemmas.\n",
       "\n",
       "- **Location:** Section 2.3.3 Conclusion and Appendix A.2 (referenced)  \n",
       "- **Verifiability:** high\n",
       "- **Search Strategy:** Access the TLA+ models and model checking results referenced in Appendix A.2 to confirm correctness and coverage of the formal verification.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## âœ… Sequential analysis completed"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Display results\n",
    "for node_id, data in result[\"operation_results\"].items():\n",
    "    if isinstance(data, types.Outline):\n",
    "        md_content = f\"\"\"\n",
    "## ðŸ“„ Document Structure ({node_id})\n",
    "\n",
    "**Topic:** {data.topic}\n",
    "\n",
    "### Key Sections:\n",
    "\"\"\"\n",
    "        for item in data.items[:3]:  # Show first 3\n",
    "            md_content += f\"- **{item.heading}**: {item.summary}\\n\"\n",
    "\n",
    "        display(Markdown(md_content))\n",
    "\n",
    "    elif isinstance(data, ClaimExtraction):\n",
    "        md_content = f\"\"\"\n",
    "## ðŸ“‘ Extracted Claims ({node_id})\n",
    "\n",
    "Found **{len(data.claims)}** verifiable claims:\n",
    "\n",
    "\"\"\"\n",
    "        for i, claim in enumerate(data.claims, 1):\n",
    "            md_content += f\"\"\"\n",
    "### {i}. [{claim.type.upper()}] {claim.claim}\n",
    "\n",
    "- **Location:** {claim.location}  \n",
    "- **Verifiability:** {claim.verifiability}\n",
    "- **Search Strategy:** {claim.search_strategy}\n",
    "\n",
    "\"\"\"\n",
    "        display(Markdown(md_content))\n",
    "\n",
    "display(Markdown(\"## âœ… Sequential analysis completed\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lionagi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
