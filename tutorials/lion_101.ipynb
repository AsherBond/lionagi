{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lionagi version \t 0.3.5\n",
      "last edited date \t 2024-10-13\n"
     ]
    }
   ],
   "source": [
    "# %pip install lionagi\n",
    "\n",
    "from lionagi import __version__\n",
    "from lionfuncs import time\n",
    "\n",
    "print(\"lionagi version \\t\", __version__)\n",
    "print(\"last edited date \\t\", time(type_=\"iso\", sep=\"T\").split(\"T\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LionAGI - 101\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisites\n",
    "\n",
    "first you will need an API key from `OpenAI` or other intelligent model provider\n",
    "\n",
    "openai: https://openai.com/api/\n",
    "\n",
    "openrouter: https://openrouter.ai/docs/quick-start#quick-start\n",
    "\n",
    "save these keys into an .env file in the root directory of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. first conversation with a LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we set up a Branch with the **Intelligent Model(iModel)**, \n",
    "\n",
    "a **branch** is an orchestrator that manages the interaction of the system with the Intelligent Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lionagi import Branch, iModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4omini_config = {\n",
    "    \"provider\": \"openai\",\n",
    "    \"model\": \"gpt-4o-mini\",\n",
    "    \"api_key_schema\": \"OPENAI_API_KEY\",\n",
    "}\n",
    "\n",
    "gpt4o_config = {\n",
    "    \"provider\": \"openai\",\n",
    "    \"model\": \"gpt-4o\",\n",
    "    \"api_key_schema\": \"OPENAI_API_KEY\",\n",
    "}\n",
    "\n",
    "# openrouter_model_config = {\n",
    "#     \"provider\": \"openrouter\",\n",
    "#     \"model\": \"gpt-4o-mini\",\n",
    "#     \"api_key_schema\": \"OPENROUTER_API_KEY\",\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comedian = Branch(\n",
    "    system=\"a respected zoo keeper, also a comedian who knows very well about indian post modern literature\",\n",
    "    imodel=iModel(**gpt4omini_config),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = \"Tell me a 20 word story: depict karma from past thousand years to future thousand years\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In ancient times, a selfish king hoarded wealth. Centuries later, his descendants faced poverty, learning kindness reshapes destiny. Karma prevails.\n"
     ]
    }
   ],
   "source": [
    "response = await comedian.chat(instruction=prompt1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the quality of the joke is up for debate, therefore why don't we invite a few more LLMs to join the conversation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's introduce a different model to be critic of the joke\n",
    "critic = Branch(\n",
    "    system=\"As an a young professor, you are trying to get into the prestigous circle of literature criticism.\",\n",
    "    imodel=iModel(**gpt4o_config),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The story is a simplistic and predictable portrayal of karma, lacking depth and originality. It fails to explore the complexities of time and consequence. Rating: 3.5/10.0\n"
     ]
    }
   ],
   "source": [
    "# now we need to give the critic the information about the joke\n",
    "# and ask it to judge it\n",
    "\n",
    "context = {\"propmt\": prompt1, \"response\": response}\n",
    "\n",
    "critic_response1 = await critic.chat(\n",
    "    instruction=\"Harsh brief commentary on the story, also rate (1.0-10.0): \",\n",
    "    context=context,\n",
    ")\n",
    "print(critic_response1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, let us turn our attention back to comedian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for the feedback! Let’s try a more nuanced approach to karma that captures its complexities over time. Here’s a revised 20-word story:\n",
      "\n",
      "A wise sage planted seeds of compassion. Millennia later, a thriving forest flourished, nurturing souls, reminding all: actions echo eternally.\n"
     ]
    }
   ],
   "source": [
    "comedian_response2 = await comedian.chat(\n",
    "    instruction=\"Story was evaluated, here is comments.\",\n",
    "    context=critic_response1,\n",
    ")\n",
    "print(comedian_response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This revision offers a more nuanced and poetic depiction of karma, illustrating the enduring impact of positive actions. It captures the essence of interconnectedness and the timeless nature of consequences. Rating: 7.0/10.0.\n"
     ]
    }
   ],
   "source": [
    "critic_response2 = await critic.chat(\n",
    "    instruction=\"Stay true to yourself, What do you think this time?\",\n",
    "    context=comedian_response2,\n",
    ")\n",
    "print(critic_response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A tyrant's empire crumbled, forgotten. Centuries later, a humble farmer's kindness sparked a movement, shaping a future of harmony and peace.\n"
     ]
    }
   ],
   "source": [
    "critic_response3 = await critic.chat(\n",
    "    instruction=\"write the \"\n",
    "    + prompt1\n",
    "    + \" in your own words, how would you do it?\",\n",
    ")\n",
    "print(critic_response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I appreciate the evaluator's insights and the attempt to capture the essence of karma in their version. Their story effectively highlights the contrast between the tyrant's downfall and the farmer's positive impact, showcasing how actions resonate through time. \n",
      "\n",
      "My revised version aimed for a more poetic and interconnected approach, but I see how the critic's attempt brings a strong narrative arc and a clear moral lesson. Both stories have their strengths: mine leans into imagery and the cyclical nature of karma, while the critic's version presents a compelling character-driven narrative. \n",
      "\n",
      "Overall, I think both stories contribute to the theme of karma in different ways, and I welcome the opportunity to keep refining my storytelling!\n"
     ]
    }
   ],
   "source": [
    "context = {\"comments\": critic_response2, \"critic_attempt\": critic_response3}\n",
    "\n",
    "comedian_response3 = await comedian.chat(\n",
    "    instruction=\"Story got evaluated again, and the evaluator wrote a their own version basing on the original prompt. What do you think of both of these? Be honest.\",\n",
    "    context=context,\n",
    ")\n",
    "print(comedian_response3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
