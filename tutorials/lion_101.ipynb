{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LionAGI - 101\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lionagi import Branch, iModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store your API key in an environment variables (via .env file, or directly into `iModel`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imodel = iModel(\n",
    "    provider=\"openai\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    # api_key=\"sk-1234567890abcdef\",  # use the actual API key, do not hardcode it here\n",
    "    api_key_schema=\"OPENAI_API_KEY\",  # use environment variable name for the key, e.g. OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we set up a Branch with the Intelligent Model\n",
    "# a branch is an orchestrator that manages the interaction of the system with the Intelligent Model\n",
    "comedian = Branch(\n",
    "    system=\"a respected zoo keeper, also a comedian who knows very well about indian post modern literature\",\n",
    "    imodel=imodel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = \"Tell me a 20 word story: depict karma from past thousand years to future thousand years\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In ancient times, a kind act blossomed; centuries later, it returned as fortune. Future generations thrived, weaving kindness into destiny.\n"
     ]
    }
   ],
   "source": [
    "response = await comedian.chat(instruction=prompt1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the quality of the joke is up for debate, therefore why don't we invite a few more LLMs to join the conversation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's introduce a different model to be critic of the joke\n",
    "imodel1 = iModel(\n",
    "    provider=\"openai\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.2,\n",
    "    api_key_schema=\"OPENAI_API_KEY\",\n",
    ")\n",
    "\n",
    "\n",
    "critic = Branch(\n",
    "    system=\"As an a young professor, you are trying to get into the prestigous circle of literature criticism.\",\n",
    "    imodel=imodel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commentary: This story, while attempting to encapsulate the vastness of time and the cyclical nature of karma, ultimately falls flat due to its overly simplistic narrative and lack of depth. The concept of kindness as a linear progression lacks the complexity that karma embodies. The prose feels rushed and fails to evoke a strong emotional response. \n",
      "\n",
      "Rating: 4.0/10.0\n"
     ]
    }
   ],
   "source": [
    "# now we need to give the critic the information about the joke\n",
    "# and ask it to judge it\n",
    "\n",
    "context = {\"propmt\": prompt1, \"response\": response}\n",
    "\n",
    "critic_response1 = await critic.chat(\n",
    "    instruction=\"Harsh brief commentary on the story, also rate (1.0-10.0): \",\n",
    "    context=context,\n",
    ")\n",
    "print(critic_response1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, let us turn our attention back to comedian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for the feedback! Letâ€™s try again with a more nuanced approach to karma that captures its complexity and depth.\n",
      "\n",
      "---\n",
      "\n",
      "In ancient India, a sage saved a wounded bird. A millennium later, its descendants healed a king. In the future, harmony thrived, echoing kindness through time.\n",
      "\n",
      "---\n",
      "\n",
      "How does this version resonate with you?\n"
     ]
    }
   ],
   "source": [
    "comedian_response2 = await comedian.chat(\n",
    "    instruction=\"Story was evaluated, here is comments.\",\n",
    "    context=critic_response1,\n",
    ")\n",
    "print(comedian_response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commentary: This revision offers a more intricate portrayal of karma, effectively illustrating the interconnectedness of actions across time. The narrative's progression from the sage's kindness to the eventual healing of a king adds layers to the concept of karma, showcasing its ripple effect through generations. However, while the imagery is evocative, the brevity still limits the emotional weight of the story. A deeper exploration of the characters' motivations and the consequences of their actions could enhance its impact.\n",
      "\n",
      "Rating: 6.5/10.0\n"
     ]
    }
   ],
   "source": [
    "critic_response2 = await critic.chat(\n",
    "    instruction=\"Stay true to yourself, What do you think this time?\",\n",
    "    context=comedian_response2,\n",
    ")\n",
    "print(critic_response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A peasant shared his harvest; centuries later, his generosity inspired a leader. In the future, communities flourished, united by compassion.\n"
     ]
    }
   ],
   "source": [
    "critic_response3 = await critic.chat(\n",
    "    instruction=\"write the \"\n",
    "    + prompt1\n",
    "    + \" in your own words, how would you do it?\",\n",
    ")\n",
    "print(critic_response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I appreciate the evaluator's insights and the effort put into their own version. Their critique highlights the importance of depth and emotional resonance, which are crucial in storytelling, especially when dealing with complex themes like karma.\n",
      "\n",
      "Comparing both stories, I see that my attempt captures the essence of karma but may still lack the emotional weight and character exploration that the evaluator's version provides. Their narrative effectively emphasizes the ripple effect of kindness through time, which is a key aspect of karma.\n",
      "\n",
      "Overall, I think both stories have their strengths and weaknesses. My version offers a glimpse into the cyclical nature of karma, while the critic's attempt presents a more cohesive and emotionally engaging narrative. I appreciate the constructive feedback and will strive to incorporate more depth and character motivation in future stories!\n"
     ]
    }
   ],
   "source": [
    "context = {\"comments\": critic_response2, \"critic_attempt\": critic_response3}\n",
    "\n",
    "comedian_response3 = await comedian.chat(\n",
    "    instruction=\"Story got evaluated again, and the evaluator wrote a their own version basing on the original prompt. What do you think of both of these? Be honest.\",\n",
    "    context=context,\n",
    ")\n",
    "print(comedian_response3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
