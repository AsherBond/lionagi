{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lionagi version \t 0.3.5\n",
      "last edited date \t 2024-10-13\n"
     ]
    }
   ],
   "source": [
    "# %pip install lionagi\n",
    "\n",
    "from lionagi import __version__\n",
    "from lionfuncs import time\n",
    "\n",
    "print(\"lionagi version \\t\", __version__)\n",
    "print(\"last edited date \\t\", time(type_=\"iso\", sep=\"T\").split(\"T\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LionAGI - 103\n",
    "\n",
    "Reflective Iterative editing of a piece of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lionagi import iModel, Branch\n",
    "\n",
    "gpt4o_config = {\n",
    "    \"provider\": \"openai\",\n",
    "    \"model\": \"gpt-4o\",\n",
    "    \"api_key_schema\": \"OPENAI_API_KEY\",\n",
    "}\n",
    "\n",
    "gpt4o = iModel(**gpt4o_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing custom logic to LLM flow\n",
    "\n",
    "---\n",
    "\n",
    "Now we can do something more complex other than having LLMs following through a pre-defined flow. We can introduce custom logic to the LLM flow. \n",
    "\n",
    "But first let's take a look at `structured_output`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM works by taking a bunch of texts and generate a bunch of texts. Natural Language Processing (NLP) isn't necessarily easy to to help us make use of the response, and one way to mitigate that is **structured output**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "going back to the comedian example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comedian_system_message = \"a respected zoo keeper, also a comedian who knows very well about indian post modern literature\"\n",
    "critic_system_message = \"As an a young professor, you are trying to get into the prestigious circle of literature criticism.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "branch = Branch(system=comedian_system_message, imodel=gpt4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure, here's a light-hearted take:\n",
       "\n",
       "A blue whale and a big white shark walk into a bar. The bartender looks up and says, \"Wow, we don't get many of your kind in here!\" The blue whale replies, \"Well, we heard the dance floor was big enough for us to really make a splash!\"\n",
       "\n",
       "The shark nods and adds, \"Yeah, and I heard the drinks are killer!\"\n",
       "\n",
       "Then they both hit the dance floor, and the whale starts doing the wave while the shark shows off its fin-tastic moves. The crowd goes wild, and someone shouts, \"Now that's what I call a whale of a time!\"\n",
       "\n",
       "And the bartender just shakes his head, thinking, \"I guess this is what happens when you mix a little liquid courage with a lot of ocean motion!\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt1 = \"Tell a joke: a blue whale met a big white shark in a bar, then started dancing.\"\n",
    "\n",
    "prompt2 = \"Tell a joke: an elephant knocked off a banana tree.\"\n",
    "\n",
    "response = await branch.chat(prompt1)\n",
    "printmd(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now intead of asking comedian to give a plain joke, let's instruct it to also give a few other things, by giving an example to the comedian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = {\n",
    "    \"category\": \"jokes\",\n",
    "    \"topic\": \"assigning personalities to animals\",\n",
    "    \"interpretation\": \"The..... therefore.....\",\n",
    "    \"joke\": \"...\",\n",
    "    \"rating\": \"an integer from 1 to 10\",\n",
    "}\n",
    "\n",
    "response = await branch.chat(\n",
    "    instruction=prompt2,\n",
    "    requested_fields=example,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"category\": \"jokes\",\n",
      "    \"topic\": \"assigning personalities to animals\",\n",
      "    \"interpretation\": \"The joke plays on the idea of elephants being large and clumsy, therefore humorously exaggerating their interactions with their environment.\",\n",
      "    \"joke\": \"Why did the elephant knock off the banana tree? Because it wanted to make a smoothie, but forgot to peel out first!\",\n",
      "    \"rating\": 7\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from lionfuncs import as_readable_json\n",
    "\n",
    "str_ = as_readable_json(response)\n",
    "\n",
    "print(str_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"rating\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we can see that the comedian can give us a joke, a joke type, and a joke rating. By making the output a `dict` we can effectively use the output of the comedian. \n",
    "\n",
    "We asked it to give itself a rating, here is an example to use it: \n",
    "\n",
    "ask comedian to repeat fixing this joke until it reaches **8/10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lionfuncs import to_num, to_dict\n",
    "\n",
    "\n",
    "async def write_a_scored_joke(prompt, idx):\n",
    "    response = await branch.chat(\n",
    "        instruction=prompt,\n",
    "        requested_fields=example,\n",
    "    )\n",
    "    response = to_dict(response)\n",
    "    response[\"rating\"] = to_num(response.get(\"rating\", 0))\n",
    "\n",
    "    str_ = f\"**Joke No.{idx+1}**:  \\n\\n \" + response[\"joke\"]\n",
    "    str_ += \"\\n\\n **Interpretation**:  \\n\\n \" + response[\"interpretation\"]\n",
    "    str_ += \"\\n\\n **Rating**:  \\n\\n \" + str(response[\"rating\"])\n",
    "\n",
    "    printmd(str_)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "async def reflective_joke(prompt, max_attempts=3):\n",
    "\n",
    "    response = await write_a_scored_joke(prompt, 0)\n",
    "    ctr = 1\n",
    "\n",
    "    while response[\"rating\"] < 8 and ctr < max_attempts:\n",
    "        printmd(\" \\n **Not a great joke, trying again** \\n\\n --- \\n \")\n",
    "        response = await write_a_scored_joke(prompt, ctr)\n",
    "        ctr += 1\n",
    "\n",
    "    if response[\"rating\"] >= 8:\n",
    "        printmd(\n",
    "            \"  \\n **Great joke!** \\n\\n --- \\n End of the joke session \\n\\n \"\n",
    "        )\n",
    "    else:\n",
    "        printmd(\n",
    "            \"  \\n **No great jokes found** \\n\\n --- \\n End of the joke session \\n\\n \"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Joke No.1**:  \n",
       "\n",
       " Why did the elephant knock off the banana tree? It was just trying to reach for a snack, but ended up going bananas!\n",
       "\n",
       " **Interpretation**:  \n",
       "\n",
       " The joke highlights the elephant's size and strength, therefore humorously suggesting it unintentionally causes chaos.\n",
       "\n",
       " **Rating**:  \n",
       "\n",
       " 6.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " \n",
       " **Not a great joke, trying again** \n",
       "\n",
       " --- \n",
       " "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Joke No.2**:  \n",
       "\n",
       " Why did the elephant knock off the banana tree? It just wanted to shake things up a bit!\n",
       "\n",
       " **Interpretation**:  \n",
       "\n",
       " The joke plays on the elephant's notorious size and strength, therefore humorously suggesting that even a simple action can lead to unintended consequences.\n",
       "\n",
       " **Rating**:  \n",
       "\n",
       " 7.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " \n",
       " **Not a great joke, trying again** \n",
       "\n",
       " --- \n",
       " "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Joke No.3**:  \n",
       "\n",
       " Why did the elephant knock off a banana tree? It was just trying to pick a banana, but ended up with the whole bunch!\n",
       "\n",
       " **Interpretation**:  \n",
       "\n",
       " The joke uses the elephant's size and strength to create a humorous scenario, therefore suggesting that even a gentle action can have big effects.\n",
       "\n",
       " **Rating**:  \n",
       "\n",
       " 8.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "  \n",
       " **Great joke!** \n",
       "\n",
       " --- \n",
       " End of the joke session \n",
       "\n",
       " "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await reflective_joke(prompt2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
